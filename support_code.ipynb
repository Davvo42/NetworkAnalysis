{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FROM TEXT TO NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From text to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-elaborazione del testo\n",
    "def preprocess_text(text):\n",
    "    # Suddividi in sezioni usando \"/\"\n",
    "    sections = text.split(\"/\")\n",
    "    # Pulisci e tokenizza ogni sezione\n",
    "    nested_lists = []\n",
    "    for section in sections:\n",
    "        # Rimuovi punteggiatura ma lascia numeri\n",
    "        section = re.sub(r\"[^\\w\\s]\", \"\", section).strip().lower()\n",
    "        # Tokenizza le parole\n",
    "        words = word_tokenize(section)\n",
    "        # Aggiungi la lista di parole alla lista generale\n",
    "        nested_lists.append(words)\n",
    "    return nested_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\Davvy\\OneDrive\\Desktop\\polimi\\erasmus\\AAA_CURSOS\\RSC\\practica\\P4\\Elon_statements.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "nested_list = preprocess_text(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contare le parole più frequenti\n",
    "def find_most_frequent_words(nested_list, top_n=30):\n",
    "    # Appiattire la lista nidificata in una singola lista di parole\n",
    "    all_words = [word for sublist in nested_list for word in sublist]\n",
    "    # Calcolare la frequenza delle parole\n",
    "    word_counts = Counter(all_words)\n",
    "    # Ottenere le parole più frequenti\n",
    "    most_common = word_counts.most_common(top_n)\n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the unwanted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(nested_list, words_to_remove):\n",
    "    # Convertire le parole da rimuovere in minuscolo per un confronto insensibile al maiuscolo/minuscolo\n",
    "    words_to_remove = set(word.lower() for word in words_to_remove)\n",
    "    # Filtrare ogni sotto-lista\n",
    "    filtered_nested_list = [\n",
    "        [word for word in sublist if word.lower() not in words_to_remove]\n",
    "        for sublist in nested_list\n",
    "    ]\n",
    "    return filtered_nested_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = ['the', 'to', 'a', 'and', 'of', 'in', 'i', 'they', 'an', 'just', 'on', 'will', 'that',\n",
    "                   'for', 'your', 'we', 'how', 'she', 'n4', 'n3', 'n2', 'n1', 'o31', 'o30', 'o29', 'o28', \n",
    "                   'with', 'o27', 'o26', 'o25', 'when', 'what', 'whatever', 'it', 'yeah', 'this', 'you', 'if', 'by', 'there',\n",
    "                   'so', 'now', 'but', 'all', 'who', 'more', 'as', 'no', 'not', 'their', 'them', 'than', 'very', 'up', 'from',\n",
    "                   'really', 'still', 'even', 'only', 'every', 'like', 'yet', 'others', 'into', 'which', 'or', 'out', 'at', 'he',\n",
    "                   'about', 'our', 'also',  'actual', 'totally', 'after', 'later', 'then', 'too', 'my', 'me', 'why', 'lot', 'much', \n",
    "                   'us', 'while', 'actually', 'many', 'some', 'v', 'where', 'again', 'however', 'over', 'anyone', 'any', 'herself', 'him',\n",
    "                   'without', 'another', 'around', 't']\n",
    "\n",
    "verbs_to_remove = ['Im', 'theyre', 'is', 'was', 'be', 'am', 'been', 'being', 'have', 'has', 'was', 'were', 'had', 'are', 'can', 'cant', 'should', 'could', 'would',\n",
    "                   'its', 'do', 'did', 'dont', 'does', 'whats', 'lets', 'doesnt']\n",
    "\n",
    "optional_verbs_to_remove = ['know', 'make', 'say', 'get', 'want', 'go', 'leave']\n",
    "\n",
    "filtered_list = remove_words(nested_list, words_to_remove)\n",
    "filtered_list = remove_words(filtered_list, verbs_to_remove)\n",
    "\n",
    "#for idx, sublist in enumerate(filtered_list, start=1):\n",
    "#    print(f\"List {idx}: {sublist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove 1 occurrence word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_occurrence_words(nested_list):\n",
    "    # Appiattire la lista nidificata in una singola lista di parole\n",
    "    all_words = [word for sublist in nested_list for word in sublist]\n",
    "    # Contare la frequenza di ogni parola\n",
    "    word_counts = Counter(all_words)\n",
    "    # Filtrare ogni sotto-lista per escludere parole con una sola occorrenza\n",
    "    filtered_nested_list = [\n",
    "        [word for word in sublist if word_counts[word] > 1]\n",
    "        for sublist in nested_list\n",
    "    ]\n",
    "    return filtered_nested_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Funzione per trasformare i verbi all'infinito\n",
    "def lemmatize_verbs_in_nested_list(nested_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_list = []\n",
    "    for sublist in nested_list:\n",
    "        tagged = nltk.pos_tag(sublist)  # Tagging grammaticale\n",
    "        lemmatized_sublist = [\n",
    "            lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) if get_wordnet_pos(tag) == wordnet.VERB else word\n",
    "            for word, tag in tagged\n",
    "        ]\n",
    "        lemmatized_list.append(lemmatized_sublist)\n",
    "    return lemmatized_list\n",
    "\n",
    "def singularize_nouns_in_nested_list(nested_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    singularized_list = []\n",
    "    for sublist in nested_list:\n",
    "        tagged = nltk.pos_tag(sublist)  # Tagging grammaticale\n",
    "        singularized_sublist = [\n",
    "            lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) if get_wordnet_pos(tag) == wordnet.NOUN else word\n",
    "            for word, tag in tagged\n",
    "        ]\n",
    "        singularized_list.append(singularized_sublist)\n",
    "    return singularized_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substitue words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {\n",
    "    \"harris\": \"kamala\",\n",
    "    \"bidens\": \"biden\",\n",
    "    \"trump\": \"donaldtrump\",\n",
    "    'easier' : 'easy',\n",
    "    'harder' : 'hard',\n",
    "    'dem'   : 'dems',\n",
    "    '1' : 'one',\n",
    "    'illegals' : 'illegal',\n",
    "    'democrat' : 'dems'\n",
    "}\n",
    "\n",
    "# Funzione per applicare le sostituzioni alla lista nidificata\n",
    "def replace_words_in_nested_list(nested_list, replacements):\n",
    "    transformed_list = [\n",
    "        [replacements.get(word, word) for word in sublist]  # Sostituisce se la parola è nel dizionario\n",
    "        for sublist in nested_list\n",
    "    ]\n",
    "    return transformed_list\n",
    "\n",
    "# Dizionario di sostituzioni di frasi\n",
    "merges = {\n",
    "    \"new york time\": \"newyorktimes\",\n",
    "    \"swing voter\" : \"swingvoter\",\n",
    "    \"citizen journalism\" : \"citizenjournalism\",\n",
    "    \"swing state\" : \"swingstate\",\n",
    "    \"democratic party\" : \"dems\",\n",
    "    \"nov 5th\" : \"nov5th\",\n",
    "    \"joe rogan\" : \"joerogan\",\n",
    "    \"north carolina\" : \"northcarolina\",\n",
    "    \"legacy medium\" : \"legacymedia\",\n",
    "    \"legacy journalism\" : \"legacyjournalism\",\n",
    "    \"legacy broadcast network\" : \"legacybroadcastnet\",\n",
    "    \"left wing\" : \"leftwing\",\n",
    "    \"democrat party\" : \"dems\",\n",
    "    \"democrats\" : \"dems\",\n",
    "    \"mind virus\" : \"mindvirus\"\n",
    "    # Aggiungi qui altre frasi da unire\n",
    "}\n",
    "\n",
    "def merge_phrases_in_nested_list(nested_list, merges):\n",
    "    transformed_list = []\n",
    "    \n",
    "    for sublist in nested_list:\n",
    "        merged_sublist = []\n",
    "        i = 0\n",
    "        while i < len(sublist):\n",
    "            # Prova a prendere una frase lunga quanto la frase nel dizionario\n",
    "            for phrase in merges:\n",
    "                # Se la frase è trovata in una sequenza di parole\n",
    "                if sublist[i:i+len(phrase.split())] == phrase.split():\n",
    "                    merged_sublist.append(merges[phrase])  # Sostituisci con la forma unita\n",
    "                    i += len(phrase.split())  # Avanza il contatore di parole\n",
    "                    break\n",
    "            else:\n",
    "                # Se nessuna frase viene trovata, aggiungi la parola singola\n",
    "                merged_sublist.append(sublist[i])\n",
    "                i += 1\n",
    "        \n",
    "        transformed_list.append(merged_sublist)\n",
    "    \n",
    "    return transformed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singolarizzare e infinitizzare\n",
    "singularized_list = singularize_nouns_in_nested_list(filtered_list)\n",
    "infinite_list = lemmatize_verbs_in_nested_list(singularized_list)\n",
    "# Unire e rimpiazzare parole\n",
    "transformed_list = replace_words_in_nested_list(infinite_list, replacements)\n",
    "merged_list = merge_phrases_in_nested_list(transformed_list, merges)\n",
    "\n",
    "\n",
    "merged_list = remove_words(merged_list, words_to_remove)\n",
    "merged_list = remove_words(merged_list, verbs_to_remove)\n",
    "No_sing_wlist = remove_single_occurrence_words(merged_list)\n",
    "\n",
    "#optional\n",
    "No_sing_wlist = remove_words(No_sing_wlist, optional_verbs_to_remove)\n",
    "\n",
    "# Filtrare le liste vuote o di una sola parola\n",
    "filtered_wlist = [sublist for sublist in No_sing_wlist if len(sublist) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a file with words and occurencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with frequencies saved to C:\\Users\\Davvy\\OneDrive\\Desktop\\polimi\\erasmus\\AAA_CURSOS\\RSC\\practica\\P4\\words_occurrencies.txt\n"
     ]
    }
   ],
   "source": [
    "word_counts = Counter(word for sublist in filtered_wlist for word in sublist)\n",
    "\n",
    "# Sort words by frequency (descending) and alphabetically (if frequencies match)\n",
    "sorted_words = sorted(word_counts.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "# File path to save the results\n",
    "output_path = r\"C:\\Users\\Davvy\\OneDrive\\Desktop\\polimi\\erasmus\\AAA_CURSOS\\RSC\\practica\\P4\\words_occurrencies.txt\"\n",
    "\n",
    "# Save words and their frequencies to a text file\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for word, count in sorted_words:\n",
    "        file.write(f\"{word}: {count}\\n\")\n",
    "\n",
    "print(f\"Words with frequencies saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a file with the final list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered list saved to C:\\Users\\Davvy\\OneDrive\\Desktop\\polimi\\erasmus\\AAA_CURSOS\\RSC\\practica\\P4\\filtered_list.txt\n"
     ]
    }
   ],
   "source": [
    "# Path where the file will be saved\n",
    "output_path = r\"C:\\Users\\Davvy\\OneDrive\\Desktop\\polimi\\erasmus\\AAA_CURSOS\\RSC\\practica\\P4\\filtered_list.txt\"\n",
    "\n",
    "# Save the filtered list to a text file\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for sublist in filtered_wlist:\n",
    "        file.write(\" \".join(sublist) + \"\\n\")\n",
    "\n",
    "print(f\"Filtered list saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the 2 excels codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'Musk_nodes.xlsx' salvato in: C:\\Users\\Davvy\\OneDrive\\Desktop\\polimi\\erasmus\\AAA_CURSOS\\RSC\\practica\\P4\\Musk_nodes.xlsx\n",
      "File 'Musk_edges.xlsx' salvato in: C:\\Users\\Davvy\\OneDrive\\Desktop\\polimi\\erasmus\\AAA_CURSOS\\RSC\\practica\\P4\\Musk_edges.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Conta la frequenza delle parole in filtered_wlist\n",
    "word_counts = Counter(word for sublist in filtered_wlist for word in sublist)\n",
    "\n",
    "# Ordina le parole per frequenza decrescente\n",
    "sorted_words = sorted(word_counts.items(), key=lambda x: -x[1])\n",
    "\n",
    "# Crea il DataFrame con Id e Label per il file Nodes\n",
    "nodes_df = pd.DataFrame({\n",
    "    'Id': range(1, len(sorted_words) + 1),\n",
    "    'Label': [word for word, count in sorted_words]\n",
    "})\n",
    "\n",
    "# Mappa delle parole agli ID\n",
    "word_to_id = {word: id for id, word in zip(nodes_df['Id'], nodes_df['Label'])}\n",
    "\n",
    "# Percorso del file Excel per Nodes\n",
    "nodes_output_path = r\"C:\\Users\\Davvy\\OneDrive\\Desktop\\polimi\\erasmus\\AAA_CURSOS\\RSC\\practica\\P4\\Musk_nodes.xlsx\"\n",
    "\n",
    "# Salva il DataFrame Nodes in un file Excel\n",
    "nodes_df.to_excel(nodes_output_path, index=False)\n",
    "\n",
    "print(f\"File 'Musk_nodes.xlsx' salvato in: {nodes_output_path}\")\n",
    "\n",
    "# Dizionario per memorizzare gli archi con pesi\n",
    "edges = defaultdict(int)\n",
    "\n",
    "# Scorriamo ogni sublista nella lista filtrata\n",
    "for sublist in filtered_wlist:\n",
    "    for i in range(len(sublist) - 1):\n",
    "        # Cattura la parola corrente e la successiva\n",
    "        source, target = sublist[i], sublist[i + 1]\n",
    "        # Ordina le parole per trattarle come non dirette\n",
    "        source, target = sorted([source, target])\n",
    "        # Mappa le parole agli ID\n",
    "        source_id = word_to_id[source]\n",
    "        target_id = word_to_id[target]\n",
    "        # Incrementa il peso per questo arco (source <-> target)\n",
    "        edges[(source_id, target_id)] += 1\n",
    "\n",
    "# Converti il dizionario in una lista di tuple\n",
    "edges_list = [(source_id, target_id, weight) for (source_id, target_id), weight in edges.items()]\n",
    "\n",
    "# Crea il DataFrame per gli edges con Source, Target e Weight\n",
    "edges_df = pd.DataFrame(edges_list, columns=['Source', 'Target', 'Weight'])\n",
    "\n",
    "# Percorso del file Excel per Edges\n",
    "edges_output_path = r\"C:\\Users\\Davvy\\OneDrive\\Desktop\\polimi\\erasmus\\AAA_CURSOS\\RSC\\practica\\P4\\Musk_edges.xlsx\"\n",
    "\n",
    "# Salva il DataFrame Edges in un file Excel\n",
    "edges_df.to_excel(edges_output_path, index=False)\n",
    "\n",
    "print(f\"File 'Musk_edges.xlsx' salvato in: {edges_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
